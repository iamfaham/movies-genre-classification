{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamfaham/movies-genre-classification/blob/main/IMDB_Genre_Prediction_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjxDH4ngkmou"
      },
      "outputs": [],
      "source": [
        "# reading the dataset\n",
        "import json\n",
        "import pandas as pd\n",
        "df = pd.read_json(\"tmdb_movies_combined.json\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB5yo9bhk963"
      },
      "outputs": [],
      "source": [
        "# checking for dataset shape\n",
        "print(\"Number of rows: \", df.shape[0])\n",
        "print(\"Number of columns: \", df.shape[1])\n",
        "# looking for redundant records\n",
        "print(\"Number of duplicate records:\", int(df.duplicated(keep = \"first\").sum()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9S1vtTzmZ4C"
      },
      "outputs": [],
      "source": [
        "# dropping duplicate records\n",
        "df.drop_duplicates(keep = \"first\", inplace = True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkdA0n2Gmjqp"
      },
      "outputs": [],
      "source": [
        "# checking for missing values\n",
        "import seaborn as sns\n",
        "sns.heatmap(df.isna(), cbar = False, yticklabels = False, cmap = \"viridis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wRpdpFoneCy"
      },
      "outputs": [],
      "source": [
        "# removing missing values\n",
        "df.dropna(inplace = True)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf5LlMYonq7O"
      },
      "outputs": [],
      "source": [
        "# checking for value counts in primary_genre\n",
        "print(\"Value Counts:\\n\", df[\"primary_genre\"].value_counts())\n",
        "df[\"primary_genre\"].value_counts().plot(kind = \"bar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3aQlzLHn4Bk"
      },
      "outputs": [],
      "source": [
        "# üî¥ Drop the \"others\" genre and rebuild X, y\n",
        "# ------------------------------------------------\n",
        "# 1. Keep rows whose primary_genre is NOT \"others\"\n",
        "mask = ~df[\"primary_genre\"].str.lower().eq(\"others\")\n",
        "df   = df.loc[mask].reset_index(drop=True)\n",
        "\n",
        "# 2. Re-create feature matrix or embedding arrays\n",
        "#    (replace `make_features(df)` with your own feature-building call)\n",
        "# X = make_features(df)          # e.g. concatenated text & image embeddings\n",
        "# y = df[\"primary_genre\"].values # target labels now have no \"others\"\n",
        "print(f\"New dataset size : {len(df):,} movies\")\n",
        "# print(\"Remaining genres  :\", pd.Series(y).unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_HJLKSDu71G"
      },
      "outputs": [],
      "source": [
        "# performing random undersampling\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "undersampler = RandomUnderSampler()\n",
        "newDf, target = undersampler.fit_resample(df.drop(\"primary_genre\", axis = 1), df[\"primary_genre\"])\n",
        "df = pd.concat([newDf, target], axis = 1)\n",
        "df[\"primary_genre\"].value_counts().plot(kind = \"bar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peW3Cxbj-3_q"
      },
      "outputs": [],
      "source": [
        "# analyzing description texts\n",
        "df['overview'].head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnXRbiA3k3CE"
      },
      "outputs": [],
      "source": [
        "# setting up NLTK\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BXdXwewk8Zd"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "nltk.download(\"wordnet\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def cleanText(text: str):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator).lower()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsA2Xmi3qAws"
      },
      "outputs": [],
      "source": [
        "df[\"overview\"] = df[\"overview\"].apply(cleanText)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2Ztk2AblWEU"
      },
      "outputs": [],
      "source": [
        "# checking for distribution of word tokens present in the descriptions\n",
        "tokenCounts = df[\"overview\"].apply(lambda x: len(nltk.word_tokenize(text = x)))\n",
        "display(tokenCounts.describe())\n",
        "display(sns.kdeplot(x = tokenCounts).set(title = \"Distribution of Token Counts of Descriptions\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRU6ERT41y0O"
      },
      "outputs": [],
      "source": [
        "# checking for a sample image\n",
        "import requests\n",
        "from PIL import Image\n",
        "import io\n",
        "Image.open(io.BytesIO(requests.get(df[\"poster_url\"].iloc[0]).content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_moVfeQYVkxT"
      },
      "outputs": [],
      "source": [
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPb7jMCAri8a"
      },
      "outputs": [],
      "source": [
        "# ‚ñ∏‚ñ∏ Poster dataloader (streams ‚â§BATCH_SIZE images at a time) ‚óÇ‚óÇ\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import requests, io, torch, gc\n",
        "from torchvision import transforms\n",
        "import io\n",
        "\n",
        "\n",
        "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "IMG_BATCH   = 32                       # drop to 16 if you still OOM\n",
        "\n",
        "# class PosterDS(Dataset):\n",
        "#     def __init__(self, urls):\n",
        "#         self.urls = urls                # just the strings, tiny in RAM\n",
        "\n",
        "#     def __len__(self):  return len(self.urls)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         url = self.urls[idx]\n",
        "#         try:\n",
        "#             img_bytes = requests.get(url, timeout=5).content\n",
        "#             img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
        "#             return img\n",
        "#         except Exception:\n",
        "#             return None                 # let collate_fn skip bad images\n",
        "\n",
        "\n",
        "class PosterDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, urls, transform =None):\n",
        "        self.urls      = urls\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.urls)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        url = self.urls[idx]\n",
        "        try:\n",
        "            resp = requests.get(url, timeout=5)\n",
        "            resp.raise_for_status()                 # catch 404/500 early\n",
        "            img = Image.open(io.BytesIO(resp.content)).convert(\"RGB\")  # <-- BytesIO\n",
        "            img = self.transform(img)\n",
        "            return idx, img\n",
        "        except Exception:\n",
        "            return None            # signals failure\n",
        "\n",
        "\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     return [img for img in batch if img is not None]\n",
        "\n",
        "# ‚¨áÔ∏é define right after the dataset, before DataLoader\n",
        "def poster_collate(batch):\n",
        "    batch = [item for item in batch if item is not None]     # drop failures\n",
        "    if not batch:                                            # whole batch failed\n",
        "        return torch.empty(0), []                            # imgs, idxs\n",
        "    idxs, imgs = zip(*batch)                                 # unzip\n",
        "    imgs = torch.stack(imgs, 0)\n",
        "    return imgs, idxs\n",
        "\n",
        "\n",
        "poster_loader = DataLoader(\n",
        "    # PosterDS(df[\"poster_url\"].tolist(), transform=poster_tf ),\n",
        "    PosterDS(df[\"poster_url\"].tolist() ),\n",
        "    batch_size = IMG_BATCH,\n",
        "    shuffle    = False,\n",
        "    num_workers=2,\n",
        "    collate_fn = poster_collate,\n",
        "    pin_memory = (DEVICE==\"cuda\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytbodT26m16o"
      },
      "outputs": [],
      "source": [
        "# using mixedbread-ai/mxbai-embed-large-v1 embeddings with embedding dimension as 1024\n",
        "from sentence_transformers import SentenceTransformer\n",
        "textEncoderModel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", truncate_dim = 1024).to(\"cuda\")\n",
        "# textEmbeddings = textEncoderModel.encode(df[\"overview\"].tolist(), normalize_embeddings=True)\n",
        "textEmbeddings = textEncoderModel.encode(\n",
        "    df[\"overview\"].tolist(),\n",
        "    normalize_embeddings = True,\n",
        "    batch_size           = 64,          # 32‚Äì128 ‚Üí tune to your GPU\n",
        "    convert_to_numpy     = True,\n",
        "    device               = DEVICE\n",
        ")\n",
        "torch.cuda.empty_cache(); gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0YohwrZyZ3a"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor, AutoModel\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# setup\n",
        "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\", trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\"microsoft/resnet-50\", trust_remote_code=True).to(\"cuda\")\n",
        "model.eval()\n",
        "\n",
        "\n",
        "embeddings = []\n",
        "success_idx  = []\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for imgs, idxs in tqdm(poster_loader):   # imgs, idxs instead of just imgs\n",
        "        if not idxs:                         # batch with all failures\n",
        "            continue\n",
        "\n",
        "        inputs = processor(images=imgs, return_tensors=\"pt\").to(DEVICE)\n",
        "        feats  = model(**inputs).last_hidden_state[:, 0]      # (B, 2048)\n",
        "\n",
        "        embeddings.append(feats.cpu())\n",
        "        success_idx.extend(idxs)              # record true dataframe rows\n",
        "imageEmbeddings = torch.cat(embeddings).numpy()             # (‚âà60 000, 2048)\n",
        "gc.collect()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS4KIKl641X-"
      },
      "outputs": [],
      "source": [
        "feat_dim = imageEmbeddings.shape[1]\n",
        "imageEmbeddings_full = np.zeros((len(df), feat_dim), dtype=imageEmbeddings.dtype)\n",
        "imageEmbeddings_full[np.array(success_idx)] = imageEmbeddings\n",
        "imageEmbeddings = imageEmbeddings_full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZNPco2XsgPS"
      },
      "outputs": [],
      "source": [
        "imageEmbeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqQlshynQ3Ip"
      },
      "outputs": [],
      "source": [
        "print(\"imageEmbeddings shape:\", imageEmbeddings.shape)   # expect (60000, 2048)\n",
        "print(\"textEmbeddings  shape:\", textEmbeddings.shape)    # expect (60000, 768)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCTb2D7vpU9K"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Create y from your dataframe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df[\"genre_id\"] = le.fit_transform(df[\"primary_genre\"])   # add new int column\n",
        "y = df[\"genre_id\"].values                                # 1-D numpy array\n",
        "\n",
        "print(\"y shape:\", y.shape)\n",
        "print(\"class ‚Ü¶ id mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG8iwlaZo4Pr"
      },
      "outputs": [],
      "source": [
        "# === TEXT-ONLY LSTM CLASSIFIER =========================================\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def train_lstm_text(X_text, y, epochs=15, hidden=256, batch=128, lr=1e-3):\n",
        "    \"\"\"\n",
        "    X_text : np.ndarray  (N, seq_len, embed_dim)  OR  (N, embed_dim)\n",
        "             If 2-D, we treat each vector as a sequence of length 1.\n",
        "    y      : np.ndarray  integer labels\n",
        "    \"\"\"\n",
        "    # Ensure 3-D: (N, seq_len, embed_dim)\n",
        "    if X_text.ndim == 2:\n",
        "        X_text = X_text[:, None, :]\n",
        "    seq_len, embed_dim = X_text.shape[1], X_text.shape[2]\n",
        "    n_classes          = int(y.max() + 1)\n",
        "\n",
        "    Xtr, Xval, ytr, yval = train_test_split(\n",
        "        X_text, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    ds_tr  = TensorDataset(torch.tensor(Xtr, dtype=torch.float32),\n",
        "                           torch.tensor(ytr, dtype=torch.long))\n",
        "    ds_val = TensorDataset(torch.tensor(Xval, dtype=torch.float32),\n",
        "                           torch.tensor(yval, dtype=torch.long))\n",
        "    dl_tr  = DataLoader(ds_tr,  batch_size=batch, shuffle=True)\n",
        "    dl_val = DataLoader(ds_val, batch_size=batch)\n",
        "\n",
        "    class TextLSTM(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.lstm  = nn.LSTM(embed_dim, hidden, num_layers=2,\n",
        "                                 batch_first=True, dropout=0.3, bidirectional=False)\n",
        "            self.fc    = nn.Linear(hidden, n_classes)\n",
        "        def forward(self, x):\n",
        "            _, (h_n, _) = self.lstm(x)           # h_n shape: (num_layers, B, hidden)\n",
        "            out = self.fc(h_n[-1])               # use last layer‚Äôs hidden state\n",
        "            return out\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model  = TextLSTM().to(device)\n",
        "    opt    = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    crit   = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        for xb, yb in dl_tr:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            loss = crit(model(xb), yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        if epoch % 5 == 0 or epoch == epochs:\n",
        "            model.eval()\n",
        "            all_pred, all_true = [], []\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in dl_val:\n",
        "                    xb = xb.to(device)\n",
        "                    pred = model(xb).argmax(1).cpu()\n",
        "                    all_pred.extend(pred)\n",
        "                    all_true.extend(yb)\n",
        "            print(f\"[Epoch {epoch}] val acc = \"\n",
        "                  f\"{(torch.tensor(all_pred) == torch.tensor(all_true)).float().mean():.4f}\")\n",
        "\n",
        "    print(\"\\nTEXT-ONLY LSTM ‚Äì validation report\")\n",
        "    print(classification_report(all_true, all_pred))\n",
        "    return model\n",
        "\n",
        "lstm_text_model = train_lstm_text(textEmbeddings, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2ZzKSHqo9rJ"
      },
      "outputs": [],
      "source": [
        "# === IMAGE-ONLY MLP CLASSIFIER =========================================\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def train_mlp_image(X_img, y, epochs=20, batch=256, lr=1e-3):\n",
        "    \"\"\"\n",
        "    X_img : np.ndarray  (N, embed_dim)\n",
        "    y     : np.ndarray  integer labels\n",
        "    \"\"\"\n",
        "    in_dim       = X_img.shape[1]\n",
        "    n_classes    = int(y.max() + 1)\n",
        "\n",
        "    Xtr, Xval, ytr, yval = train_test_split(\n",
        "        X_img, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    ds_tr  = TensorDataset(torch.tensor(Xtr, dtype=torch.float32),\n",
        "                           torch.tensor(ytr, dtype=torch.long))\n",
        "    ds_val = TensorDataset(torch.tensor(Xval, dtype=torch.float32),\n",
        "                           torch.tensor(yval, dtype=torch.long))\n",
        "    dl_tr  = DataLoader(ds_tr,  batch_size=batch, shuffle=True)\n",
        "    dl_val = DataLoader(ds_val, batch_size=batch)\n",
        "\n",
        "    class ImageMLP(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Linear(in_dim, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.4),\n",
        "                nn.Linear(512, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.3),\n",
        "                nn.Linear(256, n_classes)\n",
        "            )\n",
        "        def forward(self, x): return self.net(x)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model  = ImageMLP().to(device)\n",
        "    opt    = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    crit   = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        for xb, yb in dl_tr:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            loss = crit(model(xb), yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        if epoch % 5 == 0 or epoch == epochs:\n",
        "            model.eval()\n",
        "            all_pred, all_true = [], []\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in dl_val:\n",
        "                    xb = xb.to(device)\n",
        "                    pred = model(xb).argmax(1).cpu()\n",
        "                    all_pred.extend(pred)\n",
        "                    all_true.extend(yb)\n",
        "            print(f\"[Epoch {epoch}] val acc = \"\n",
        "                  f\"{(torch.tensor(all_pred) == torch.tensor(all_true)).float().mean():.4f}\")\n",
        "\n",
        "    print(\"\\nIMAGE-ONLY MLP ‚Äì validation report\")\n",
        "    print(classification_report(all_true, all_pred))\n",
        "    return model\n",
        "\n",
        "mlp_image_model = train_mlp_image(imageEmbeddings, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqSHUmTyRAz9"
      },
      "outputs": [],
      "source": [
        "# Convert (59990, 7, 7)  ‚Üí  (59990, 49)\n",
        "# imageEmbeddings = imageEmbeddings.reshape(imageEmbeddings.shape[0], -1)\n",
        "print(\"Flattened imageEmbeddings:\", imageEmbeddings.shape)   # (59990, 49)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K8j2D38ylsA"
      },
      "outputs": [],
      "source": [
        "# Should be >0 if real embeddings are present\n",
        "non_zero_rows = np.count_nonzero(np.linalg.norm(imageEmbeddings, axis=1) > 0)\n",
        "print(\"Rows with real poster features:\", non_zero_rows)    # probably prints 0 right now\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wlcRCCdI_nX"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# === Input: Choose your feature set ===\n",
        "# X = textEmbeddings  # or imageEmbeddings or fused\n",
        "# GOOD: row-wise concatenation ‚Üí 2-D\n",
        "X = np.concatenate([imageEmbeddings, textEmbeddings], axis=1)   # (60000, 2816)\n",
        "y = df[\"primary_genre\"].values\n",
        "\n",
        "# === Optional: PCA (to reduce overfitting in high-dim) ===\n",
        "use_pca = True\n",
        "n_components = 256  # Tune this (e.g., 50, 100, 200)\n",
        "\n",
        "# === Train/Val Split ===\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# === Normalize ===\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# === Apply PCA if needed ===\n",
        "if use_pca:\n",
        "    pca = PCA(n_components=n_components, random_state=42)\n",
        "    X_train = pca.fit_transform(X_train)\n",
        "    X_val = pca.transform(X_val)\n",
        "\n",
        "# === Logistic Regression with Regularization ===\n",
        "# C is inverse of regularization: lower C = stronger regularization\n",
        "clf = LogisticRegression(\n",
        "    C=0.1,  # try 0.01 or 0.001 for stronger regularization\n",
        "    max_iter=1000,\n",
        "    solver='lbfgs',\n",
        "    multi_class='multinomial'\n",
        ")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Predict ===\n",
        "y_train_pred = clf.predict(X_train)\n",
        "y_val_pred = clf.predict(X_val)\n",
        "\n",
        "# === Accuracy ===\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "val_acc = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"‚úÖ Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"‚úÖ Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "# === Reports ===\n",
        "print(\"\\nüìä Train Classification Report:\")\n",
        "print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "print(\"\\nüìä Validation Classification Report:\")\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "# === Confusion Matrix ===\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(y_val, y_val_pred), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Validation Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-44UGoWf5gZ"
      },
      "outputs": [],
      "source": [
        "# stacking images and text embeddings horizontally\n",
        "embeddings = np.hstack((imageEmbeddings, textEmbeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5msEReRqiaMO"
      },
      "outputs": [],
      "source": [
        "# encoding the target feature\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# encoder = LabelEncoder()\n",
        "# df[\"primary_genre\"] = encoder.fit_transform(df[\"primary_genre\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhyV6BLJiEs0"
      },
      "outputs": [],
      "source": [
        "# using a train-test-split for confusion matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(embeddings, df[\"primary_genre\"], test_size=0.20, random_state=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5RdJNirQTy9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "poly_lr = make_pipeline(\n",
        "    PolynomialFeatures(degree=3, include_bias=False),  # cubic terms & interactions\n",
        "    StandardScaler(with_mean=False),                  # keep sparse matrix compact\n",
        "    LogisticRegression(\n",
        "        penalty=\"l2\",\n",
        "        C=1.0,\n",
        "        max_iter=200,\n",
        "        multi_class=\"multinomial\",\n",
        "        n_jobs=-1,\n",
        "        class_weight=\"balanced\"\n",
        "    )\n",
        ")\n",
        "\n",
        "poly_lr.fit(x_train, y_train)\n",
        "y_pred = poly_lr.predict(X_val)\n",
        "print(\"Poly-LR val accuracy:\", accuracy_score(y_val, y_pred))\n",
        "print(classification_report(y_val, y_pred, digits=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eA6H3smURxfM"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Encode primary_genre strings ‚Üí 0 ‚Ä¶ num_classes-1  ‚îÄ‚îÄ\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# import pandas as pd, gc\n",
        "\n",
        "# label_encoder = LabelEncoder()\n",
        "# y = label_encoder.fit_transform(df[\"primary_genre\"])   # numpy int64 array\n",
        "# print(\"Unique genres:\", list(label_encoder.classes_))  # optional\n",
        "\n",
        "# # keep a DataFrame column if you need it later\n",
        "# df[\"genre_id\"] = y\n",
        "# gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wY6OG76RnwJ"
      },
      "outputs": [],
      "source": [
        "# ---- Sanity-check & repair the genre labels ----\n",
        "import numpy as np, torch, gc\n",
        "\n",
        "# y is the numpy / pandas column you just created\n",
        "num_classes = len(np.unique(y))\n",
        "print(\"Detected #genres:\", num_classes)\n",
        "\n",
        "# 1. make sure labels start at 0 and are consecutive\n",
        "y_min, y_max = y.min(), y.max()\n",
        "assert y_min >= 0, f\"Found negative label ({y_min})\"\n",
        "assert y_max < num_classes, f\"Label {y_max} ‚â• num_classes-1 ({num_classes-1})\"\n",
        "\n",
        "# 2. cast to int64 (PyTorch 'long')\n",
        "y = y.astype(\"int64\")\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOKmFF56hFN1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert to tensors (if not already done earlier)\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# PyTorch Logistic Regression Model\n",
        "class PyTorchLogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(PyTorchLogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.linear(x), dim=1)\n",
        "\n",
        "num_classes = len(np.unique(y_train))      # y_train is the 1-D int64 array\n",
        "print(\"Num classes:\", num_classes)         # ‚Üí should print 7\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logreg_model = PyTorchLogisticRegression(x_train.shape[1], num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(logreg_model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Training loop\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "best_val_acc = 0\n",
        "patience = 3\n",
        "wait = 0\n",
        "\n",
        "for epoch in range(20):  # Increased epochs\n",
        "    logreg_model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = logreg_model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        correct += (preds.argmax(dim=1) == yb).sum().item()\n",
        "\n",
        "    train_acc = correct / len(train_loader.dataset)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Validation accuracy\n",
        "    logreg_model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = logreg_model(xb)\n",
        "            correct += (preds.argmax(dim=1) == yb).sum().item()\n",
        "    val_acc = correct / len(test_loader.dataset)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}] Loss: {total_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        wait = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ2ZJu-R8UB6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_accuracies)+1), train_accuracies, label=\"Train Accuracy\")\n",
        "plt.plot(range(1, len(val_accuracies)+1), val_accuracies, label=\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Logistic Regression Accuracy over Epochs\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Z8S3hgXkYSZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Evaluation\n",
        "logreg_model.eval()\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for xb, _ in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        preds = logreg_model(xb)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "\n",
        "predicted = np.argmax(np.vstack(all_preds), axis=1)\n",
        "\n",
        "# üí° Make sure we tell sklearn the label order\n",
        "cm = confusion_matrix(y_test, predicted, labels=range(len(encoder.classes_)))\n",
        "\n",
        "labels = encoder.classes_\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix (Logistic Regression)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X694ymPtqZ6"
      },
      "outputs": [],
      "source": [
        "print(\"Unique y_test values:\", np.unique(y_test))\n",
        "print(\"Encoder classes:\", encoder.classes_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9el_hI6Movd"
      },
      "outputs": [],
      "source": [
        "# standardizing the inputs\n",
        "# Convert to torch tensors first\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
        "\n",
        "# Compute mean and std from training data\n",
        "mean = x_train_tensor.mean(dim=0, keepdim=True)\n",
        "std = x_train_tensor.std(dim=0, keepdim=True)\n",
        "\n",
        "# Avoid division by zero\n",
        "std[std == 0] = 1\n",
        "\n",
        "# Normalize\n",
        "x_train_tensor = (x_train_tensor - mean) / std\n",
        "x_test_tensor = (x_test_tensor - mean) / std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIamS346MWDS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Convert data to tensors\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# Define the PyTorch ANN model\n",
        "class GenreClassifier(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(GenreClassifier, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.bn4 = nn.BatchNorm1d(32)\n",
        "        self.dropout4 = nn.Dropout(0.3)\n",
        "\n",
        "        self.output = nn.Linear(32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout3(x)\n",
        "        x = F.relu(self.bn4(self.fc4(x)))\n",
        "        x = self.dropout4(x)\n",
        "        return F.log_softmax(self.output(x), dim=1)\n",
        "\n",
        "\n",
        "# Instantiate and train\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GenreClassifier(x_train.shape[1]).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "best_val_acc = 0\n",
        "patience = 5\n",
        "wait = 0\n",
        "\n",
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        correct += (preds.argmax(dim=1) == yb).sum().item()\n",
        "\n",
        "    train_acc = correct / len(train_loader.dataset)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = model(xb)\n",
        "            correct += (preds.argmax(dim=1) == yb).sum().item()\n",
        "\n",
        "    val_acc = correct / len(test_loader.dataset)\n",
        "    val_accuracies.append(val_acc)\n",
        "    train_losses.append(total_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:2d} | Loss: {total_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        wait = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"‚èπ Early stopping triggered.\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNz_XR37N5x-"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(train_accuracies)+1), train_accuracies, label='Train Accuracy')\n",
        "plt.plot(range(1, len(val_accuracies)+1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('ANN Accuracy over Epochs')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eN8s0tNQX6j"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Loss over epochs\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_losses, label='Train Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy over epochs\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, val_accuracies, label='Val Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation Accuracy over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDMO00Q1RCyl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get predictions for test set\n",
        "model.eval()\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for xb, _ in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        preds = model(xb)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "\n",
        "predicted = np.argmax(np.vstack(all_preds), axis=1)\n",
        "cm = confusion_matrix(y_test, predicted)\n",
        "\n",
        "labels = le.classes_\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix (ANN)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKjpBKoITXmw"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        preds = model(xb)\n",
        "        correct += (preds.argmax(dim=1) == yb).sum().item()\n",
        "\n",
        "final_acc = correct / len(test_loader.dataset)\n",
        "print(f\"Final Test Accuracy: {final_acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}